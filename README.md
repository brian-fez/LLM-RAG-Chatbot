# ğŸ¤– Document RAG Chat

Document RAG Chat is a lightweight RAG-based chatbot built with **Streamlit** that allows you to upload and chat with any PDF document. It uses **HuggingFace embeddings**, a **FAISS vector store**, and a **local LLM** (like Ollama's deepseek-r1:8b) to provide accurate and contextual answers.

---

## âœ¨ Features

- ğŸ“‚ Upload a PDF and ask questions about its content
- ğŸ” Uses semantic search over embedded chunks
- ğŸ’¬ Chat interface with session history and saving
- ğŸ§  Hidden reasoning (`<think>` sections) logged but not shown in UI
- ğŸ§¹ "New Chat" clears chat, state, and uploaded documents

---

## ğŸ“ Project Structure

```
document-rag-chat/
â”‚
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ chatbot.py          # Chatbot logic (LLM + vector store)
â”œâ”€â”€ vectors.py          # PDF loading, chunking, embedding
â”œâ”€â”€ utilities.py        # Helpers: session, logging, cleanup
â”‚
â”œâ”€â”€ chat_sessions/      # Saved chat history (.json)
â”œâ”€â”€ chat_logs/          # Full LLM logs with reasoning
â”œâ”€â”€ temp/               # Temporary PDF storage
â”‚
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
```

---

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/brian-fez/LLM-RAG-Chatbot.git
   cd LLM-RAG-Chatbot
   ```

2. **(Optional) Create a virtual environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸš€ Running the App

To start the chatbot interface:

```bash
streamlit run app.py
```

Then open your browser to the URL shown (usually `http://localhost:8501`).

---

## ğŸ§  How It Works

1. **PDF Upload**: You upload a document.
2. **Embedding**: The text is split into chunks and embedded with a HuggingFace model (e.g., `BAAI/bge-small-en-v1.5`).
3. **Vector Store**: The chunks are saved in a FAISS index.
4. **Retrieval + Generation**:
   - Relevant chunks are retrieved based on your question.
   - An LLM (local or remote) generates a response using the chunks as context.
5. **Display**:
   - The full output (including `<think>` hidden reasoning) is logged to disk.
   - Only the clean, visible part is shown in the UI.

---

## ğŸ’¾ Sessions & Logs

- ğŸ’¬ Chat messages are saved in `/chat_sessions/` as `.json`.
- ğŸ§  Full LLM outputs are saved in `/chat_logs/` with timestamps.
- ğŸ“‚ Uploaded PDFs are temporarily stored in `/temp/` and removed when starting a new chat.

---

## âœ… Requirements

- Python 3.9+
- Streamlit
- FAISS
- HuggingFace Transformers & Sentence Transformers
- Optional: Ollama (for local LLM like `deepseek`)

See `requirements.txt` for full dependencies.

---

## ğŸ§¼ Future Improvements

- ğŸ—‚ï¸ Multi-document support
- ğŸ›¡ï¸ Authentication
- ğŸŒ Deploy to Streamlit Cloud or Hugging Face Spaces
- ğŸ³ Docker container for production
- ğŸ’¬ Support chat history export

---